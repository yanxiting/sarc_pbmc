---
title: "GSEA analysis of SARC associated genes"
author: "Xiting Yan"
date: "10/02/2019"
output:
  html_notebook:
    code_folding: hide
    fig_caption: yes
    highlight: tango
    number_sections: no
    theme: united
    toc: yes
    toc_depth: 6
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval=TRUE,echo = TRUE,cache=TRUE,warning=FALSE,message = FALSE,results='hold',cache.lazy = FALSE)
knitr::opts_knit$set(eval.after = 'fig.cap',dev=c('png','postscript'))


library(captioner)
library(circlize)
library(clusterProfiler)
library(corrplot)
library(ComplexHeatmap)
library(dplyr)
library(EnsDb.Hsapiens.v86)
library(gdata)
library(ggplot2)
library(ggpubr)
library(ggrepel)
library(gplots)
library(grid)
library(gridExtra)
library(gridGraphics)
library(kableExtra)
library(knitr)
library(Matrix)
library(org.Hs.eg.db)
library(parallel)
library(reshape2)
library(scales)
library(Seurat)
library(SingleR)
library(tidyr)
library(tidyverse)
library(viridisLite)
library(xlsx)
library(randomcoloR)
library(kableExtra)
library(gdata)
library(knitr)
library(captioner)
library(nlme)
library(rgl)
library(gplots)
library(WGCNA)
library(xlsx)
library(randomcoloR)
library(ComplexHeatmap)

knit_hooks$set(webgl = hook_webgl)


table_nums_1 <- captioner::captioner(prefix="Table",levels=1)
figure_nums_1<- captioner::captioner(prefix="Figure",levels=1)


table_nums_2 <- captioner::captioner(prefix="Table",levels=2)
figure_nums_2<- captioner::captioner(prefix="Figure",levels=2)

table_nums_3 <- captioner::captioner(prefix="Table",levels=3)
figure_nums_3 <- captioner::captioner(prefix="Figure",levels=3)

home.dir<-"/home/yanxiting/driver_Grace"
#home.dir<-"/home/xy48"
#source(paste(home.dir,"/Rprogram/my_functions.R",sep=""))
```
# Data Downloading

## sratools
We downloaded the data using SRAtools on grace using the scripts generated by the following codes. We downloaded the sratools from github and installed it on grace with setup. All files downloaded were not zipped so we also zipped each file into fastq.gz file.

```{r}
id.list<-readLines("/Users/yanxiting/Downloads/SRR_Acc_List(2).txt")
temp1<-paste("/home/xy48/scratch_palmer/sratoolkit/sratoolkit.3.0.0-ubuntu64/bin/prefetch.3.0.0 -X 9999999999999 ",id.list,"\n",sep="")
temp2<-paste("/home/xy48/scratch_palmer/sratoolkit/sratoolkit.3.0.0-ubuntu64/bin/fastq-dump.3.0.0 --split-files ./",id.list,".sra\n",sep="")
temp3<-rep("cd /home/xy48/scratch_palmer/SARC_10x/test/sra\n",length(temp1))
temp4<-paste("gzip ./",id.list,"_*.fastq\n",sep="")
temp5<-rep("ssh transfer\n",length(temp1))
cmd.out<-cbind(temp5,temp1,temp3,temp2,temp4)
temp.cmd<-apply(cmd.out,1,paste,collapse="",sep="")
cat(temp.cmd,file="./download_commands.txt",append=F,sep="")

```

## changing names
The cell ranger pipeline recognize fastq file names are formated as [Sample Name]_S1_L00[Lane Number]_[Read Type]_001.fastq.gz. There are 3 files per sample representing I1, R1 and R2 downloaded by sratools, which were named as *_1, *_2 and *_3.fastq files. To correctly run cell ranger on these files, we need to change the fastq.gz names.

```{r}
# this was run on grace.
source("~/Rprogram/my_functions.R")
data.dir<-"/home/xy48/scratch_palmer/SARC_10x/test/sra"
filenames<-list.files(data.dir)
temp<-sapply(filenames,my.element.extract,splitchar="\\.",index=3)
filenames<-filenames[temp=="gz"]
filenames<-filenames[!is.na(filenames)]

for(i in 1:length(filenames)){
  
  if(substr(filenames[i],1,4)=="SRR9"){
  	from.filename<-filenames[i]
  	temp<-my.element.extract(filenames[i],splitchar="\\.",index=1)
  	temp<-my.element.extract(temp,splitchar="_",index=2)
  	if(temp=="1"){
  		to.filename<-paste0(my.element.extract(filenames[i],splitchar="_",index=1),"_S1_L001_I1_001.fastq.gz")
  	}
  	if(temp=="2"){
  		to.filename<-paste0(my.element.extract(filenames[i],splitchar="_",index=1),"_S1_L001_R1_001.fastq.gz")
  	}
  	if(temp=="3"){
  		to.filename<-paste0(my.element.extract(filenames[i],splitchar="_",index=1),"_S1_L001_R2_001.fastq.gz")
  	}
  
  	file.rename(from=file.path(data.dir,from.filename),to=file.path(data.dir,to.filename))
  	cat("from=",file.path(data.dir,from.filename),"\n","to=",file.path(data.dir,to.filename),"\n",sep="")
  }else{
    
  	from.filename<-filenames[i]
  	temp<-my.element.extract(filenames[i],splitchar="\\.",index=1)
  	temp<-my.element.extract(temp,splitchar="_",index=2)
  	if(temp=="1"){
  		to.filename<-paste0(my.element.extract(filenames[i],splitchar="_",index=1),"_S1_L001_R1_001.fastq.gz")
  	}
  	if(temp=="2"){
  		to.filename<-paste0(my.element.extract(filenames[i],splitchar="_",index=1),"_S1_L001_R2_001.fastq.gz")
  	}
  	if(temp=="3"){
  		to.filename<-paste0(my.element.extract(filenames[i],splitchar="_",index=1),"_S1_L001_I1_001.fastq.gz")
  	}
  
  	file.rename(from=file.path(data.dir,from.filename),to=file.path(data.dir,to.filename))
  	cat("from=",file.path(data.dir,from.filename),"\n","to=",file.path(data.dir,to.filename),"\n",sep="")    
    
  }
}
```

## cell ranger


We rsync the fastq.gz files onto farnam under ~/scratch60/SARC_10X/fastq and run cell ranger on the files using commands generated by the following codes.

```{r}
id.list<-readLines("/Users/yanxiting/Downloads/SRR_Acc_List(2).txt")
temp1<-paste("#!/bin/bash\n#SBATCH --time=24:00:00  --ntasks=1 --partition=general --cpus-per-task=8 --mem=80GB --job-name=",id.list," -o /home/xy48/scratch60/SARC_10X/cellranger_scripts/",id.list,".sh.o%J -e /home/xy48/scratch60/SARC_10X/cellranger_scripts/",id.list,".sh.e%J\n",sep="")
temp2<-rep("module load cellranger/5.0.0\nmodule load bcl2fastq2/2-20-0-foss-2018b\n",length(id.list))
temp3<-paste("cd /home/xy48/scratch60/SARC_10X/cellranger_results\n")
temp4<-paste("cellranger count --id=",id.list," --fastqs=/home/xy48/scratch60/SARC_10X/fastq --transcriptome=/home/xy48/scratch60/SARC_10X/refdata-gex-GRCh38-2020-A --localcores=8 --localmem=60 --sample=",id.list,"\n",sep="")
cmd.out<-cbind(temp1,temp2,temp3,temp4)
temp.cmd<-apply(cmd.out,1,paste,collapse="",sep="")
cat(temp.cmd,file="./cellranger_scripts.txt",append=F,sep="")
```


There are samples with multiple runs. We run cellranger aggr to merge the multiple runs into one single nUMI vector.

```{r}
source(file.path(home.dir,"Rprogram/my_functions.R"))
library(xlsx)
home.dir<-"/home/yanxiting/driver_Farnam"
runtable.filepath<-file.path(home.dir,"scratch60/SARC_10X/SraRunTable.txt")
id.filepath<-file.path(home.dir,"scratch60/SARC_10X/scRNA_ids.xlsx")

run.table<-read.table(runtable.filepath,sep="\t",header=T,check.names = F)
id.table<-read.xlsx(id.filepath,sheetIndex = 1,check.names=F)

temp.table<-run.table[run.table$`GEO_Accession (exp)`%in%as.matrix(id.table)[,1],]
temp.list<-split(as.matrix(temp.table)[,1],temp.table$`GEO_Accession (exp)`)

# extract the samples with multiple runs
temp.list<-temp.list[unlist(lapply(temp.list,length))>1]

# find out the fastq part these IDs belong to
temp.filenames<-list.files(file.path(home.dir,"scratch60/SARC_10X"))
temp.filenames<-temp.filenames[grep(".filelist",temp.filenames)]
temp.filenames<-temp.filenames[grep("fastq",temp.filenames)]

file.list<-list()
for(i in 1:length(temp.filenames)){
  temp<-readLines(file.path(home.dir,"scratch60/SARC_10X",temp.filenames[i]))
  temp<-unname(sapply(temp,my.element.extract,splitchar="/",index=-1))
  temp<-unname(sapply(temp,my.element.extract,splitchar="_",index=1))
  temp<-unique(temp)
  file.list[[i]]<-temp
}
names(file.list)<-temp.filenames
temp<-character()
for(i in 1:length(file.list)){
  temp<-c(temp,rep(names(file.list)[i],length(file.list[[i]])))
}

file.list.matrix<-cbind(unlist(file.list),temp)

temp<-list()
for(i in 1:length(temp.list)){
  temp[[i]]<-unique(file.list.matrix[file.list.matrix[,1]%in%temp.list[[i]],2])
}

# transfer the cellranger results back to farnam
# generate the aggregation CSV file
cellranger.dir<-file.path(home.dir,"scratch60/SARC_10X/cellranger_results")
output.dir<-file.path(home.dir,"scratch60/SARC_10X/cellranger_aggr_scripts")

for(i in 1:length(temp.list)){
  output.filepath<-file.path(output.dir,paste(names(temp.list)[i],"_aggr.csv",sep=""))
  # pipeline before cellranger 6.0, use library. Otherwise, use sample_id
  cmd.out<-c("library_id","molecule_h5")
  cmd.out<-rbind(cmd.out,cbind(temp.list[[i]],paste("/home/xy48/scratch60/SARC_10X/cellranger_results/",temp.list[[i]],"/outs/molecule_info.h5",sep="")))
  write.table(cmd.out,file=output.filepath,row.names=F,col.names=F,sep=",",append=F,quote=F)
}

# generate the sh file to run cellranger aggr on the replicated runs of the same sample.
script.dir<-file.path(home.dir,"scratch60/SARC_10X/cellranger_aggr_scripts")
result.dir<-file.path(home.dir,"scratch60/SARC_10X/cellranger_aggr_results")

for(i in 1:length(temp.list)){
  csv.filepath<-file.path(script.dir,paste(names(temp.list)[i],"_aggr.csv",sep=""))
  script.filepath<-file.path(script.dir,paste(names(temp.list)[i],".sh",sep=""))
  cmd.out<-paste("#!/bin/bash\n#SBATCH --time=24:00:00  --ntasks=1 --partition=general --cpus-per-task=1 --mem=80GB --job-name=",names(temp.list)[i]," -o /home/xy48/scratch60/SARC_10X/cellranger_aggr_scripts/",names(temp.list)[i],".sh.o%J -e /home/xy48/scratch60/SARC_10X/cellranger_aggr_scripts/",names(temp.list)[i],".sh.e%J\n",sep="")
  cmd.out<-paste(cmd.out,"module load cellranger/5.0.0\nmodule load bcl2fastq2/2-20-0-foss-2018b\n",sep="")
  cmd.out<-paste(cmd.out,"cd /home/xy48/scratch60/SARC_10X/cellranger_aggr_results\n",sep="")
  cmd.out<-paste(cmd.out,"cellranger aggr --id=",names(temp.list)[i]," --csv=/home/xy48/scratch60/SARC_10X/cellranger_aggr_scripts/",names(temp.list)[i],"_aggr.csv\n",sep="")
  cat(cmd.out,file=script.filepath,append=F)
}

```

We moved the cellranger aggr results back to cellranger_results together with the samples with unique run.

## UMI matrix extraction

We extract the nUMI vector for the samples included in the original paper.

```{r}
home.dir<-"/home/yanxiting/driver_Farnam"
source(file.path(home.dir,"Rprogram/my_functions.R"))
library(xlsx)
runtable.filepath<-file.path(home.dir,"scratch60/SARC_10X/SraRunTable.txt")
id.filepath<-file.path(home.dir,"scratch60/SARC_10X/scRNA_ids.xlsx")

run.table<-read.table(runtable.filepath,sep="\t",header=T,check.names = F)
id.table<-read.xlsx(id.filepath,sheetIndex = 1,check.names=F)

my.table<-run.table[run.table$`GEO_Accession (exp)`%in%as.matrix(id.table)[,1],]

filenames<-list.files(file.path(home.dir,"scratch60/SARC_10X/cellranger_results"))

merged.data<-numeric()
for(i in 1:length(filenames)){
  if(substr(filenames[i],1,1)=="G"){
    matrix.dir<-file.path(home.dir,"scratch60/SARC_10X/cellranger_results",filenames[i],"outs","count","filtered_feature_bc_matrix")
  }else{
    matrix.dir<-file.path(home.dir,"scratch60/SARC_10X/cellranger_results",filenames[i],"outs","filtered_feature_bc_matrix")
  }
  
  barcode.path <- file.path(matrix.dir, "barcodes.tsv.gz")
  features.path <- file.path(matrix.dir, "features.tsv.gz")
  matrix.path <- file.path(matrix.dir, "matrix.mtx.gz")
  mat <- readMM(file = matrix.path)
  feature.names = read.delim(features.path,
                             header = FALSE,
                             stringsAsFactors = FALSE)
  barcode.names = read.delim(barcode.path,
                             header = FALSE,
                             stringsAsFactors = FALSE)
  if(substr(filenames[i],1,1)=="G"){
    sample.name<-filenames[i]
  }else{
    sample.name<-as.matrix(my.table)[my.table$Run==filenames[i],"Sample Name"]
  }
  
  colnames(mat) = paste0(sample.name,"_",barcode.names$V1)
  rownames(mat) = feature.names$V1
  
  if(i==1){
    merged.data<-mat
    gene.names<-rownames(mat)
  }else{
    merged.data<-cbind(merged.data,mat[gene.names,])
  }
}

output.filepath<-"/home/yanxiting/driver_Farnam/scratch60/SARC_10X/merged_nUMI_45.rds"
saveRDS(merged.data,file=output.filepath,refhook=NULL)

```

We transferred the cellranger results back to farnam to extract the nUMI results. We also add the samples with multiple runs into the merged matrix.

```{r eval=FALSE}
# samples with multiple runs have names of the GEO accession number.
source("/home/yanxiting/driver_Farnam/Rprogram/my_functions.R")
library(Matrix)
cellranger.dir<-"/home/yanxiting/driver_Farnam/scratch60/SARC_10X/cellranger_results"
runtable.filepath<-"/home/yanxiting/driver_Farnam/scratch60/SARC_10X/SraRunTable.txt"

# load in the run table
run.table<-read.table(runtable.filepath,sep="\t",header=T,check.names=F,stringsAsFactors = FALSE)

# load in the run names and the GSM IDs under the cell ranger results folder
temp.filenames<-list.files(cellranger.dir)

sample.rep<-temp.filenames[substr(temp.filenames,1,3)=="GSM"]
sample.uni<-temp.filenames[substr(temp.filenames,1,3)!="GSM"]
my.run.table<-run.table[as.matrix(run.table)[,"Sample Name"]%in%sample.rep | as.matrix(run.table)[,"Run"]%in%sample.uni,]
# extract the nUMI matrix
dirnames<-list.files(cellranger.dir)
dirnames<-dirnames[substr(dirnames,1,4)=="SRR1"]

for(i in 1:length(dirnames)){
  run.name<-dirnames[i]
  sample.name<-run.table[run.table[,"Run"]==run.name,"Sample Name"]
  matrix_dir<-file.path(cellranger.dir,dirnames[i],"outs","filtered_feature_bc_matrix")
  barcode.path <- file.path(matrix_dir, "barcodes.tsv.gz")
  features.path <- file.path(matrix_dir, "features.tsv.gz")
  matrix.path <- file.path(matrix_dir, "matrix.mtx.gz")
  mat <- readMM(file = matrix.path)
  feature.names = read.delim(features.path,
                             header = FALSE,
                             stringsAsFactors = FALSE)
  barcode.names = read.delim(barcode.path,
                             header = FALSE,
                             stringsAsFactors = FALSE)
  colnames(mat) = paste0(sample.name,"_",barcode.names$V1)
  rownames(mat) = feature.names$V1
  
  output.filepath<-file.path("/home/yanxiting/driver_Farnam/scratch60/SARC_10X/cellranger_results",paste0(sample.name,"_",run.name,"_nUMI.rds"))
  saveRDS(mat,file=output.filepath,refhook=NULL)
}

```


Since we have limited storage on the hpc, we back up the fastq.gz files and the cell ranger output folders. We extracted the nUMI matrix of each sample separately and deleted all other files to make room. All fastq.gz files and their corresponding cell ranger output folders are backed up on google drive under /Volumes/GoogleDrive/My Drive/GraceBackup/GRADS/SARC_PBMC/SARC_10X.

## Seurat Analysis

We load the data into Seurat for downstream analysis, especially for the integrated analysis.

First, we create the Seurat object and preliminary filtering on genes and
```{r}
# due to the unavailability of the cluster, we ran these codes on tac4 server with the same versions of R and packages
# srun --pty --x11 -p pi_kaminski -t 12:00:00 --ntasks=1 --nodes=1 --cpus-per-task=1 --mem=60152 bash
# module restore seurat
#home.dir<-"/home/yanxiting/driver_Farnam"
home.dir<-"/home/yanxiting/Documents/Research/GRADS_SARC_PBMC"
source("/home/yanxiting/Rprogram/my_functions.R")

#output.dir<-file.path(home.dir,"scratch60/SARC_10X/Seurat")
output.dir<-file.path(home.dir,"scRNA-seq/Seurat")

if(file.exists(output.dir)==F){
dir.create(output.dir)
}

# Load the merged data
merged.data<-readRDS(file.path(home.dir,"Data/merged_nUMI_45.rds"),refhook=NULL)

# load in the genes.gtf to convert ensembl ID to gene names
gtf.filepath<-"/home/yanxiting/driver_Farnam/scratch60/SARC_10X/refdata-gex-GRCh38-2020-A/genes/genes.gtf"
gtf.matrix<-read.table(gtf.filepath,sep="\t",header=F,check.names = F, stringsAsFactors = FALSE,skip = 5)
gtf.matrix<-gtf.matrix[gtf.matrix[,3]=="gene",]
temp<-gtf.matrix[,9]

gene.id<-unname(sapply(sapply(temp,my.element.extract,splitchar="; ",index=1),my.element.extract,splitchar=" ",index=-1))
gene.name<-unname(sapply(sapply(temp,my.element.extract,splitchar="; ",index=4),my.element.extract,splitchar=" ",index=-1))
gene.list<-split(gene.id,gene.name)
# get the list of ensembl IDs from mitochondrial genes
mt.genes<-unlist(gene.list[grep("^MT-",names(gene.list))])

#geoff.data <- Read10X(data.dir = file.path(data.dir,"outs","filtered_gene_bc_matrices_mex","GRCh38"))
#load(file.path(home.dir,"scratch_kaminski/public/Backup/Jonas/R_objects/10x_ChuppAsthma.mtx.hybrid_gene_symbols_wo_background_03_1119.Robj"))
gcql1 <- CreateSeuratObject(counts = merged.data, project = "SARC_10X",min.cells = 3,  min.features = 200)
#gcql1[["percent.mt"]] <- PercentageFeatureSet(gcql1, pattern = "^MT-")
gcql1[["percent.mt"]] <- PercentageFeatureSet(gcql1, features = mt.genes)

cat("\n")
cat("Originally, there are ",nrow(merged.data)," genes and ",ncol(merged.data)," cells in the data.\n",sep="")
cat("After the first step filtering, there are ",nrow(gcql1@assays$RNA@data)," genes and ",ncol(gcql1@assays$RNA@data)," cells in the filtered data.\n",sep="")

rm(merged.data)
#gc(verbose=F)
invisible(gc())

# load in the phenotype data of all the samples
id.filepath<-file.path(home.dir,"Data/scRNA_ids.xlsx")
runtable.filepath<-file.path(home.dir,"Data/SraRunTable.txt")

run.table<-read.table(runtable.filepath,sep="\t",header=T,check.names = F)
id.table<-read.xlsx(id.filepath,sheetIndex = 1,check.names=F)

# get the sample names for all the cells
my.ident<-unname(sapply(colnames(gcql1@assays$RNA@counts),my.element.extract,splitchar="_",index=1))
names(my.ident)<-colnames(gcql1@assays$RNA@counts)

temp<-split(as.matrix(run.table)[,"study_classification"],as.matrix(run.table)[,"Sample Name"])
temp<-lapply(temp,unique)
my.disease<-unlist(temp[my.ident])

my.samplenames<-my.ident
disease.samplenames<-paste(my.disease,"_",my.samplenames,sep="")

# scRNA-seq QC metric.
#mito.genes1 <- grep(pattern = "^MT-", x = rownames(gcql1@assays$RNA@counts), value = TRUE)
#percent.mito1 <- Matrix::colSums(gcql1@assays$RNA@counts[mito.genes1, ])/Matrix::colSums(gcql1@assays$RNA@counts)

# AddMetaData adds columns to object@meta.data, and is a great place to
# stash QC stats
gcql1 <- AddMetaData(object = gcql1, metadata = my.disease, col.name = "disease")
gcql1 <- AddMetaData(object = gcql1, metadata = my.samplenames, col.name = "sample.names")
gcql1 <- AddMetaData(object = gcql1, metadata = my.ident, col.name = "sample")
gcql1 <- AddMetaData(object = gcql1, metadata = disease.samplenames, col.name = "disease.samplenames")
temp<-rep("pbmc",nrow(gcql1@meta.data))
gcql1 <- AddMetaData(object = gcql1, metadata = temp, col.name = "tissue")

# filter the data to only keep genes present (>0 in >=1% of all the cells) in >=3 subjects
gene.pres<-numeric()
sample.names.unique<-unique(gcql1@meta.data$sample.names)

for(i in 1:length(sample.names.unique)){
  temp.matrix<-gcql1@assays$RNA@counts[,gcql1@meta.data$sample.names==sample.names.unique[i]]
  temp.vect<-apply(temp.matrix>0,1,sum)
  gene.pres<-cbind(gene.pres,temp.vect>=(ncol(temp.matrix)*0.01))
}

gene.names<-rownames(gcql1@assays$RNA@counts)[apply(gene.pres,1,sum)>=3]
gcql1.orig<-gcql1
gcql1<-subset(gcql1.orig,features=gene.names)

cat("After the first and second steps of filtering, there are ",nrow(gcql1@assays$RNA@counts)," genes and ",ncol(gcql1@assays$RNA@counts)," cells in the data.\n",sep="")
```


Second, we had a third filtering step on cells based on the nGene and percent.mito. Here are the plots to help us decide the threshold of the filtering.
```{r fig.width=20,fig.height=10,cache=FALSE}

#g1<-VlnPlot(object = gcql1, features = c("nFeature_RNA", "nCount_RNA", "percent.mt"), ncol = 3,group.by="samplenames",combine=T)

g2<-VlnPlot(object = gcql1, features= c("nFeature_RNA", "nCount_RNA", "percent.mt"), ncol = 3,group.by = "disease.samplenames",combine=T)
grid.arrange(g2,ncol=1)
```


```{r fig.width=14,fig.height=4,echo=FALSE,cache=FALSE,results='asis'}
# show the 2D scatterplot
g1<-FeatureScatter(gcql1,feature1="nCount_RNA",feature2="percent.mt",group.by = "tissue",pt.size=0.5)
g1<-g1+geom_abline(intercept=10,slope=0,col="red") 

g2<-FeatureScatter(gcql1,feature1="nCount_RNA",feature2="nFeature_RNA",group.by = "tissue",pt.size=0.5)
g2<-g2+geom_abline(intercept=300,slope=0,col="red")
g2<-g2+geom_abline(intercept=6000,slope=0,col="red")

g3<-FeatureScatter(gcql1,feature1="nFeature_RNA",feature2="percent.mt",group.by = "tissue",pt.size=0.5)
g3<-g3+geom_abline(intercept=10,slope=0,col="red")
g3<-g3+geom_vline(xintercept=330,col="red")
g3<-g3+geom_vline(xintercept=6000,col="red")
g<-grid.arrange(g1,g2,g3,ncol=3)
ggsave(filename=file.path(output.dir,"1_datapreprocessing_FeatureScatter.pdf"),plot=g,device="pdf",units = "in",width=14,height=4)
```

```{r}
# only keep cells with nFeature_RNA>=330 or <=6000
#gcql1<-subset(gcql1,subset= nFeature_RNA >= 300 & nFeature_RNA<= 6000 & percent.mt<= 50)
gcql1<-subset(gcql1,subset= nFeature_RNA >= 300 & percent.mt<= 50)
cat("After the third filtering step, we have ",nrow(gcql1@assays$RNA@counts)," genes and ",ncol(gcql1@assays$RNA@counts)," cells.\n",sep="")
```
Based on the quality control plots, in the third filtering step, we removed cells with >50% mitochondrial percentage or  <300 expressed genes. After this second filtering step, we have `r nrow(gcql1@assays$RNA@counts)` genes and `r ncol(gcql1@assays$RNA@counts)` cells remained. 


Third, we normalize the unimputed and save the SAVER imputed data as the normalized data. These data are saved for downstream further processing.

```{r}
# normalize the data and save the results
gcql1 <- NormalizeData(gcql1, normalization.method = "LogNormalize",  scale.factor = 10000)
saveRDS(gcql1, file = file.path(output.dir,"1_normalized_data_unimputed_1.rds"))

#gcql1.saver<-NormalizeData(object = gcql1.saver, normalization.method = "LogNormalize",  scale.factor = 10000)
#saveRDS(gcql1.saver, file = file.path(output.dir,"preprocessed_data_saverimputed_1.rds"))

cat("the preprocessed data for the unimputed data was saved as ",file.path(output.dir,"1_normalized_data_unimputed_1.rds"),"\n",sep="")

cat("After the normalization, there are ",nrow(gcql1@assays$RNA@data)," genes and ",ncol(gcql1@assays$RNA@data)," cells\n",sep="")

#cat("the preprocessed data for the SAVER imputed data was saved as ",file.path(output.dir,"preprocessed_data_saverimputed_1.rds"),"\n",sep="")
```

### Original Cell Clustering

First, we find variable genes.
```{r}
features.n<-2000
home.dir<-"/home/yanxiting/Documents/Research/GRADS_SARC_PBMC"
```

We load in the normalized unimputed data and find the top `r features.n` variable genes for cell clustering. The normalized data was further scaled to remove the effect of nUMI and percent.mito for further cell clustering purpose.

```{r fig.width=10,fig.height=5}
# load in the normalized unimputed data
data.filepath<-file.path(home.dir,"scRNA-seq/Seurat/1_normalized_data_unimputed_1.rds")
output.dir<-file.path(home.dir,"scRNA-seq/Seurat")

if(file.exists(output.dir)==F){
  dir.create(output.dir)
}

gcql1<-readRDS(file = data.filepath, refhook = NULL)
gcql1<-FindVariableFeatures(gcql1,selection.method="vst",nfeatures=features.n)

# Identify the 10 most highly variable genes
top10 <- head(VariableFeatures(gcql1), 10)

# plot variable features with and without labels
plot1 <- VariableFeaturePlot(gcql1)
plot2 <- LabelPoints(plot = plot1, points = top10, repel = TRUE)
CombinePlots(plots = list(plot1, plot2))
```


Then we scale the data to remove the effect from nUMI and percent.mito. Here we decide to scale for all genes so that later on, the heatmap will be generated using the scaled data. 
```{r results='hide'}
#maybe regress cell cycle http://satijalab.org/seurat/cell_cycle_vignette.html
all.genes<-rownames(gcql1)
#gcql1 <- ScaleData(gcql1,features = all.genes, vars.to.regress = c("nCount_RNA", "percent.mt"))
gcql1 <- ScaleData(gcql1)
#output.filepath<-file.path(output.dir,"2_scaled_allgenes_unimputed.rds")
#saveRDS(gcql1, file = output.filepath)
```


We perform the principal component analysis on the variable genes to decide the number of PCs to use for clustering.
```{r}
gcql1 <- RunPCA(object = gcql1, features = VariableFeatures(gcql1), npcs=200,verbose=F)
```


To see if there are PCs that are subject specific, the 2D PCA plots using the top 3 PCs are as follows for the un-imputed data. 
```{r fig.width=10,fig.height=15,cache=FALSE,results='hide'}
#par(mfrow=c(2,2))
fig.1<-DimPlot(object = gcql1, dims = c(1,2), pt.size=0.5)
fig.2<-DimPlot(object = gcql1, dims = c(2,3), pt.size=0.5)
fig.3<-DimPlot(object = gcql1, dims = c(1,3), pt.size=0.5)
grid.arrange(fig.1,fig.2,fig.3,ncol=1)
```

We draw the heatmap of the top genes for each PC.
```{r fig.width=15,height=80,cache=FALSE}
DimHeatmap(gcql1, dims = 1:10, cells = 500, balanced = TRUE)
```
```{r fig.width=15,height=80,cache=FALSE}
DimHeatmap(gcql1, dims = 11:20, cells = 500, balanced = TRUE)
```

```{r fig.width=15,height=80,cache=FALSE}
DimHeatmap(gcql1, dims = 21:30, cells = 500, balanced = TRUE)
```

```{r fig.width=15,height=80,cache=FALSE}
DimHeatmap(gcql1, dims = 31:40, cells = 500, balanced = TRUE)
```

```{r fig.width=15,height=80,cache=FALSE}
DimHeatmap(gcql1, dims = 41:50, cells = 500, balanced = TRUE)
```

```{r fig.width=15,height=80,cache=FALSE}
DimHeatmap(gcql1, dims = 51:60, cells = 500, balanced = TRUE)
```

```{r fig.width=15,height=80,cache=FALSE}
DimHeatmap(gcql1, dims = 61:70, cells = 500, balanced = TRUE)
```

```{r fig.width=15,height=80,cache=FALSE}
DimHeatmap(gcql1, dims = 71:80, cells = 500, balanced = TRUE)
```

We use the following Jackstraw plots and the Elbowplot to decide the number of PCs to use for downstream analysis.
```{r fig.width=10,fig.height=40,results='hold'}
gcql1 <- JackStraw(object = gcql1, num.replicate = 100, verbose = TRUE,dims=200)
gcql1 <- ScoreJackStraw(gcql1, dims = 1:200)
```


```{r fig.width=16,fig.height=8,cache=FALSE,results='hide'}
JackStrawPlot(gcql1, dims = 1:100)
```

```{r fig.width=8,fig.height=8,cache=FALSE,results='hide'}
ElbowPlot(object = gcql1,ndims=100)
```

```{r}
pc.num<-35
```

Based on the JackStraw plot and the Elbow plot, we decided to use the top `r pc.num` PCs for the unimputed data.

```{r}
saveRDS(gcql1, file = file.path(output.dir,paste("2_dimreduction_data_",length(gcql1@assays$RNA@var.features),"vargenes_pipeline1.rds",sep="")))
# we save the file as preprocessed_data_saver.rds to prevent any mistakes but we change the file name back to preprocessed_data.rds before performing downstream analysis.

cat("We save the R project as\n")
cat("unimputed data:\t",file.path(output.dir,paste("2_dimreduction_data_",length(gcql1@assays$RNA@var.features),"vargenes_pipeline1.rds",sep="")),"\n",sep="")
```


Based on the Jackstraw plots, we decided to use the `r pc.num` top PCs for clustering. 

## Data visualization

### UMAP
To identify potential outlying cells and samples, we first label the tSNE plot using disease_samplenames.
```{r}
########################################################################
# 2. generate the TSNE plot labelled specifically for each sample separately to look for outliers.
# srun --pty --x11 -p pi_kaminski -t 12:00:00 --ntasks=1 --nodes=1 --cpus-per-task=1 --mem=60152 bash
#module load Apps/R/3.4.1-generic
#module load Apps/R/3.4.3-generic

data.filepath<-file.path(output.dir,"2_dimreduction_data_2000vargenes_pipeline1.rds")
gcql1 <- readRDS(file = data.filepath, refhook = NULL)

# generate the TSNE plot labeled by labeling samples with different colors 
sample1<-FindNeighbors(gcql1,dims=1:pc.num)
sample1<-FindClusters(sample1,resolution=1.2)
sample1<-RunUMAP(sample1,dims=1:pc.num)
#sample1<-RunUMAP(sample1,dims=1:pc.num,umap.method="umap-learn")


```

```{r fig.width=10,fig.height=20,fig.cap="UMAP labeled by a). cell clusters, b). sample names and c). disease for the unimputed data.",cache=FALSE,results='hide'}
fig1<-DimPlot(sample1,reduction="umap")
fig2<-DimPlot(sample1,reduction="umap",group.by="disease.samplenames")
fig3<-DimPlot(sample1,reduction="umap",group.by="disease")
grid.arrange(fig1,fig2,fig3,ncol=1)
```

### tSNE plot

```{r}
sample1 <- RunTSNE(sample1, dims= 1:pc.num)
```

```{r fig.width=10,fig.height=20,fig.cap="tSNE plots labeled by a). cell clusters, b). sample names and c). disease for the unimputed data.",cache=FALSE,results='hide'}
fig1<-DimPlot(sample1,reduction="tsne")
fig2<-DimPlot(sample1,reduction="tsne",group.by="disease.samplenames")
fig3<-DimPlot(sample1,reduction="tsne",group.by="disease")
grid.arrange(fig1,fig2,fig3,ncol=1)
```

### data saving
```{r}
saveRDS(sample1, file = file.path(output.dir,paste("2_visualization_data_",length(gcql1@assays$RNA@var.features),"vargenes_pipeline1.rds",sep="")))
cat("We saved the seurat object with UMAP and tSNE as ",file.path(output.dir,paste("2_visualization_data_",length(sample1@assays$RNA@var.features),"vargenes_pipeline1.rds",sep="")),"\n",sep="")
```


### Integrative Analysis

```{r}
my.distinct.colors<-c('#e6194b', '#3cb44b', '#ffe119', '#4363d8', '#f58231', '#911eb4', '#46f0f0', '#f032e6', '#bcf60c', '#fabebe', '#008080', '#e6beff', '#9a6324', '#fffac8', '#800000', '#aaffc3', '#808000', '#ffd8b1', '#000075', '#808080', '#ffffff', '#000000')
```

To describe the cell types captured in the data, the integrated analysis may provide cleaner results. Therefore, we conduct the integrated analysis of all the asthma 10X data using Seurat V3.1 in this note to reduce the subject effect in the data visulization and the trajectory analysis at least. 

# data loading
```{r data_loading,results='hold'}
########################################################
# 1. load in the singler object
data.dir<-output.dir
sample1<-readRDS(file.path(data.dir,"2_visualization_data_2000vargenes_pipeline1.rds"),refhook = NULL)

sample1.raw<-sample1
```

# integrating data

Split the whole dataset based on subject ID (fresh and DSMO samples from the same subject are considered as one sample). Find anchors to integrate all the datasets together.
```{r}
sample1.list<-SplitObject(sample1.raw,split.by="sample.names")
sample1.list<-lapply(X=sample1.list,FUN=function(x){
  x<-NormalizeData(x)
  x<-FindVariableFeatures(x,selection.method="vst",nfeatures=2000)
})
temp.cellnum<-unlist(lapply(sample1.list,ncol))
# increase the global maxSize
options(future.globals.maxSize= 4194304000)
#sample1.anchors<-FindIntegrationAnchors(sample1.list[temp.cellnum>=92],k.filter=92,dims=1:40,verbose=FALSE)
sample1.anchors<-FindIntegrationAnchors(sample1.list[temp.cellnum>=40],k.filter=40,dims=1:40,verbose=FALSE)
#sample1.combined<-IntegrateData(anchorset=sample1.anchors,features.to.integrate=rownames(sample1.raw@assays$RNA@counts),dims=1:40,verbose=FALSE)
sample1.combined<-IntegrateData(anchorset=sample1.anchors,dims=1:40,verbose=FALSE)
```


```{r data_saving}
dir.create(file.path(output.dir,"integrated_analysis"))
output.filepath<-file.path(output.dir,"integrated_analysis","seurat_object_integrated_allcells.rds")
saveRDS(sample1.combined,file=output.filepath)
cat("We saved the integrated data for all cells as ",output.filepath,"\n",sep="")

output.filepath<-file.path(output.dir,"integrated_analysis","seurat_object_original_allcells.rds")
saveRDS(sample1,file=output.filepath)
cat("We saved the original data for all cells as ",output.filepath,"\n",sep="")
```

Cluster the cells using the integrated data.

```{r}
DefaultAssay(sample1.combined) <- "integrated"
# Run the standard workflow for visualization and clustering
sample1.combined <- ScaleData(sample1.combined, verbose = FALSE)
sample1.combined <- RunPCA(sample1.combined, npcs = 40, verbose = FALSE)
# t-SNE and Clustering
sample1.combined <- RunUMAP(sample1.combined, reduction = "pca", dims = 1:40)
sample1.combined <- FindNeighbors(sample1.combined, reduction = "pca", dims = 1:40)
sample1.combined <- FindClusters(sample1.combined, resolution = 1.2)
```

We save the result in a rds file.
```{r}
output.subdir<-file.path(output.dir,"integrated_analysis")
if(file.exists(output.subdir)==F){
  dir.create(output.subdir)
}

output.filepath<-file.path(output.subdir,"seurat_object_integrated_allcells_cellclustering_noSingleR.rds")
saveRDS(sample1.combined,file=output.filepath)
cat("We saved the seurat object of the integrated data with cell clustering results only as ",output.filepath,"\n",sep="")
```


## Visualizing integrated data

```{r,fig.width=8,fig.height=20}
library(cowplot)
p1 <- DimPlot(sample1.combined, reduction = "umap", group.by = "sample.names")
p2 <- DimPlot(sample1.combined, reduction = "umap", label = TRUE)
#p3<-DimPlot(sample1.combined, reduction = "umap", group.by="singler.hpca.cluster.merged", label = TRUE,cols=my.distinct.colors)
#p4<-DimPlot(sample1.combined, reduction = "umap", group.by="singler.hpca.cluster", label = TRUE,cols=my.distinct.colors)
p5<-DimPlot(sample1.combined, reduction = "umap", group.by="disease", label = TRUE,cols=my.distinct.colors)
#plot_grid(p2, p1, p3,p4,p5,ncol=1)
plot_grid(p2, p1,p5,ncol=1)
```

# Integrate with existing data

We obtained another PBMC 10X data annotated in Kaminski lab so that we can integrate this dataset with the annotated data to annotate the cells in this dataset. The seurat object containing the annotated PBMC 10x data is under /home/xy48/scratch_palmer/Amy_PBMC on grace hpc. We ran this section of codes on hpc.

## data loading

We first load in the annotated PBMC 10x data from Amy.
```{r}
data.filepath<-"/home/yanxiting/driver_Grace/scratch_palmer/Amy_PBMC/01.03.23.controls.for.Xiting.rds"
amy.10x<-readRDS(data.filepath,refhook = NULL)
```

Visualize the integrated data by splitting it to different clusters.

```{r,fig.width=10,fig.height=50}
fig.list<-list()
celltype.names<-as.numeric(unique(as.character(Idents(sample1.combined))))
celltype.names<-sort(celltype.names,decreasing=F)
for(i in 1:length(celltype.names)){
  temp.map<-as.character(Idents(sample1.combined))
  temp.map[temp.map!=celltype.names[i]]="other"
  sample1.combined@meta.data$temp.map<-temp.map
  fig.list[[2*(i-1)+1]]<-DimPlot(sample1.combined, reduction = "umap", label = FALSE,group.by="ident")
  #fig.list[[2*i]]<-DimPlot(sample1.combined, reduction = "umap", label = FALSE,group.by="temp.map",cols=c(my.distinct.colors[i],"grey"))
  fig.list[[2*i]]<-DimPlot(sample1.combined, reduction = "umap", label = FALSE,group.by="temp.map",cols=c("red","grey"))
}

do.call(grid.arrange,c(fig.list,ncol=2))
```

Visualize the integrated data by splitting it to different subjects and different cell type annotation (old).

```{r,fig.width=16,fig.height=24}
fig.list<-list()
celltype.names<-unique(as.character(sample1.combined@meta.data$singler.hpca.cluster))
celltype.names<-sort(celltype.names,decreasing=T)
for(i in 1:length(celltype.names)){
  temp.map<-as.character(sample1.combined@meta.data$singler.hpca.cluster)
  temp.map[temp.map!=celltype.names[i]]<-"zother"
  sample1.combined@meta.data$temp.map<-temp.map
  fig.list[[i]]<-DimPlot(sample1.combined, reduction = "umap",group.by="temp.map",label = FALSE,pt.size=0.5,cols=c(my.distinct.colors[i],"grey"))
}
do.call(grid.arrange,c(fig.list,ncol=2))
```



